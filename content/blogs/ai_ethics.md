Just as a quick ahead, if you haven’t read my blog on fatalism, that is pretty relevant to the points I want to talk about here, so I’d recommend you read it! With that aside, let’s talk about some AI ethics.

I'm going to be honest, until a few months ago, I haven't exactly thought much about AI ethics, however, a mix between happenings and my own stupidity led me to consider some thoughts on this issue.

First of all, on a national, also global too, level, AI ethics has become a large issue. Between undisclosed data usage and the use of AI for very questionable purposes, people, media, and all sorts of different actors have started talking about AI ethics for real, and it was one of the big reasons I became aware of AI ethics in full.

The second, much more stupid reason, is that I had an idea to create a model to predict the future. The core concept was to translate a series of global events into a sort of embedding like many language transformer models do, except instead of words being embedded, it would be events. Then, I would feed the series of events into a transformer model and predict future events!

Looking back on it, I'm embarrassed to have even thought of this idea. It is absolutely absurd, technically nonsensical, and beyond anything else, deeply unethical. I was in a bit of a crazed state when I came up with the idea, and when I calmed down, I realized that the warnings from my friends that "yeah, this probably isn't a good idea?" was very right of them.

However, past introspections aside, how does fatalism tie into all of this? Well, I've mentioned before that: "I realized that, even if free will, thought, and subjectivity are all illusions created through emergent properties by the of physics, they are real to me."

And when looking at the majority of probability-based predictive models these days that are applied to human-related interactions, I feel a certain dissonance. Probability-based predictive models, I feel, when applied to human-related interactions, seem to validate a system of determinism, even to a point where it promotes fatalism. And, this is bad, not just in terms of the ethics of treating humans as data points, that is an issue, but not the core of it. I believe that the acceptance of and belief in the concept of free will is one of the most critical components in terms of a functioning society and a happy humanity, yet, human-applied probability-based predictive models invalidates this core belief, essentially denying people of their right to believe in free will. As someone who knows what fatalism feels like, I think it's pretty absurd!

Ethics and morality aside, I do also think the general trend of probability-based predictive models is unsustainable. While scientific innovation always begins with pattern recognition, I feel we are expanding the scope of the use of these models too quickly and too far. When we slap a transformer onto everything and say "we'll figure out the principles later", it feels like an excuse not to. After all, who would try to find the fundamentals of an issue when there is already a SOTA approach that does it perfectly well? This is an overgeneralization, however, I do feel there is a risk that we fall into this trap, where we don't focus enough on the why and how and instead focus on the what and where. 

Thus, I came to the conclusion that probability-based predictive models, especially those aimed at humans, was pretty unethical. I do have one main idea on alternatives though. I believe that a modelling-based approach to predictions would be much better. First of all, on ethics, modelling, by understanding humans instead of predicting them, promotes learning and self-improvement. The difference in being told: "I know with xx% certainty that will do yy" vs "I believe you will do yy because xx" is massive, because the second, from my perspective, allows one to see why and then think of ways to circumvent or self-improve while the first one is just a plain statement. Second of all, on sustainability, I just think that modelling is more scientifically valid. I would say that transformers are peak engineering but not peak science, and I do feel like the two are somewhat getting conflated.

BUT! This is all just a theory. An ETHICS theory! (yes i used to watch game theory, how could you tell ;D) I honestly don't know much about AI ethics and I based these ideas off of my personal experiences, so any thoughts or feedback from people more well-versed in this field would be greatly appreciated. With that aside, I shall go to bed.

Cheers!