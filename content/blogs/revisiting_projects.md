About two weeks ago, I decided to revisit a few of my older projects / projects I put on hold, those being PolyStock AI and Alitheia AI, and oh boy have things changed. It's really surprising, coming back to projects and realizing there is so much things you can do that you've never thought up of seen before. I remember reading a while back about cognitive expertise in [*Reconsidering the Trade-off Between Expertise and Flexibility: a Cognitive Entrenchment Perspective* by Erik Dane](https://journals.aom.org/doi/full/10.5465/amr.35.4.zok579), and if I do say so myself, I believe that I'm just on the cusp of falling into entrenchment, so I still have some wiggle room to squeeze out new ideas. 

Boring things aside, let's get into the fun things: what exactly changed? Since going into detail will take way too long to write, I'll focus on one big thing for every project.

For PolyStock AI, the big change I made was in terms of model architecture. A few months prior, I was watching a youtube series on how ChatGPT worked, and I noticed that the sequence to sequence system could be used to predict extended stock prices pretty well. As seq2seq puts words after words, I thought that it would be applicable to put it with stock prices, where instead of words, it would be price point after price point. Using seq2seq for time-series modeling definitely isn't a new thing, see [*Foundations of Sequence-to-Sequence Modeling for Time Series* by Kuznetsov et al](https://arxiv.org/abs/1805.03714), however, I've never seen it applied to stock data in specific, so I tried implementing it. 

It turned out much better than I would have though, see the StockPred project, and it really surprised me. Seq2seq and transformers as a whole are, first and foremost, a probability thing, so seeing it work for continuous values was unusual, however, there has been research done on using it for continuous values, so I guess it isn't too out there, see [*OT-Transformer: A Continuous-time Transformer Architecture with Optimal Transport Regularization* by Kan et al](https://arxiv.org/html/2501.18793v1). 

For Alitheia AI, the big change I made was in terms of how I optimized by clustering systems. This has always been a major problem for meâ€”finding the optimal clustering systems is very difficult, especially since you can't exactly "train" them in the conventional sense. However, while I was messing around with another random project of mine, not something that I actually wanted to put on my transcripts or anything, I found out about Optuna, a hyperparameter optimization method. A few months after that, it came to me that I could use Optuna to optimize the hyperparameters of my clustering system while using custom-generated ground truth clustering datasets.

It worked... well? While it wasn't perfect, 9 times out of 10, it would provide better clustering than arbitrary values, but there is always still that 10%. Clustering is just so unlike conventional ML systems, which makes it just as difficult to mess around with. 

For both of these concepts however, I definitely think there is a lot more that needs to be looked at. For the clustering, task-specific hyperparameter optimization could be very useful, but I don't see too much potential as it's a pretty niche task. Transformer-based continuous time-series predictions are definitely worth looking into though as they could come in handy for a lot of things, and for both of these concepts, I would love to hear any insights people have on them.

Cheers!